# Pr치ctica 2. Consumo y transformaci칩n de datos en Azure Databricks

## 游꿢 Objetivos:
Al finalizar la pr치ctica, ser치s capaz de:
- Consumir informaci칩n desde una tabla existente, evaluar su calidad y aplicar procesos de limpieza y enriquecimiento utilizando dos enfoques distintos: transformaciones con PySpark y transformaciones con SQL.
- Guardar una versi칩n limpia del dataset para usarla en an치lisis posteriores.

## 游닇 Requisitos previos:

- Haber completado la Pr치ctica 1  
- Tener acceso al cl칰ster Databricks y a la tabla `ventas`  
- Conocimientos b치sicos de PySpark y SQL

## 游 Duraci칩n aproximada:
- 45 minutos.

---

**[拘勇 Atr치s](https://netec-mx.github.io/Custom_NETEC_DBRICKS-DA_INT-Priv/Cap칤tulo1/lab1.html)** | **[Lista General](https://netec-mx.github.io/Custom_NETEC_DBRICKS-DA_INT-Priv/)** | **[Siguiente 俱뫮잺](https://netec-mx.github.io/Custom_NETEC_DBRICKS-DA_INT-Priv/Cap칤tulo3/lab3.html)**

---

## Instrucciones:

### Tarea 1: Creaci칩n o verificaci칩n del servidor en Databricks y carga de los datos

#### Tarea 1.1.

- **Paso 1.** Si eliminaste tu workspace de Databricks o el cl칰ster, **repite la Tarea 1 de la pr치ctica de configuraci칩n del entorno**.

    ***游눠 NOTA:** Si ya tienes el cl칰ster creado, avanza directamente al **Paso 2**.*

    [Haz clic aqu칤 para ir a la pr치ctica: Configurar entorno individual en Azure Databricks](https://netec-mx.github.io/Custom_NETEC_DBRICKS-DA_INT-Priv/Cap칤tulo1/lab0.html)

- **Paso 2.** Accede a tu workspace de Databricks haciendo clic en el bot칩n **Launch Workspace**.

    ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img12.png)

- **Paso 3.** Descarga el dataset de demostraci칩n desde este **[enlace](https://neteclabs.blob.core.windows.net/courses/databricks/intermedio/databricks-int-dataset.csv)**.

- **Paso 4.** Haz clic en el men칰 lateral izquierdo para cargar el archivo con los datos. Luego, en la parte derecha de la pantalla, selecciona **Agregar los datos**.

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img23.png)

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img24.png)

- **Paso 5.** Haz clic en la opci칩n **Create or modify table**.

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img25.png)

- **Paso 6.** Arrastra o carga el archivo que descargaste previamente, verifica los datos y haz clic en **Create table**.

  ***游눠 NOTA:** Puede tardar unos segundos en mostrar los datos.*

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img26.png)

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img27.png)

- **Paso 7.** Haz clic en el **Workspace** desde el men칰 lateral izquierdo.

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img18.png)

- **Paso 8.** Crea un nuevo notebook con el nombre `Lab2_Processing` y selecciona el lenguaje `SQL`.

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img19.png)

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img1.png)

- **Paso 9.** Adjunta tu cl칰ster activo al notebook.

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img21.png)

- **Paso 10.** Ejecuta el siguiente comando para cambiar el nombre de la tabla a **`ventas`** para mayor facilidad.

  ```sql
  ALTER TABLE databricks_int_dataset RENAME TO ventas;
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab1/img29.png)

  ---

> **TAREA FINALIZADA**  

### Resultado esperado:
Has preparado tu ambiente para la ejecuci칩n de consultas y scripts.

---

### Tarea 2: Evaluar la calidad del dataset  

Antes de transformar los datos, es esencial entender su estado actual: su estructura, la presencia de valores nulos y registros duplicados.

#### Tarea 2.1.

- **Paso 1.** Revisa el esquema de la tabla `ventas`:  

  ```sql
  DESCRIBE TABLE ventas;
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img2.png)


- **Paso 2** Contar el total de registros:  
    
  ```python
  %python
  spark.table("ventas").count()
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img3.png)

- **Paso 3.** Contar los valores nulos por columna:
    
  ```python
  %python
  from pyspark.sql.functions import col, sum as _sum
  ventas = spark.table("ventas")
  ventas.select([_sum(col(c).isNull().cast("int")).alias(c) for c in ventas.columns]).show()
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img4.png)

- **Paso 4.** Contar los valores duplicados exactos:

  ```python
  %python
  ventas.groupBy(ventas.columns).count().filter("count > 1").count()
  ```
  ---
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img5.png)

> **TAREA FINALIZADA**

### Resultado esperado:
Tendr치s claridad sobre si es necesario limpiar valores nulos, eliminar duplicados o corregir tipos de datos.

---

### Tarea 3: Limpieza y transformaci칩n con PySpark

En esta tarea, procesar치s los datos usando PySpark para generar un nuevo DataFrame enriquecido.

#### Tarea 3.1.

- **Paso 1.** Elimina las filas que contengan valores nulos en columnas cr칤ticas.  

  ```python
  %python
  display(ventas.filter(col("PrecioUd").isNull() | col("Unidades").isNull() | col("Fecha").isNull()))
  ventas_limpias = ventas.dropna(subset=["PrecioUd", "Unidades", "Fecha"])
  display(ventas_limpias)
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img6.png)

- **Paso 2.** Elimina los duplicados exactos:

  ```python
  %python
  from pyspark.sql.functions import count
  ventas.groupBy("NumPed", "Fecha", "Articulo").count().filter("count > 1").show()
  ventas.exceptAll(ventas.dropDuplicates()).show()
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img7.png)

- **Paso 3.** Convierte la columna `Fecha` a tipo fecha: 

  ```python
  %python
  ventas.select("Fecha").distinct().show(5)
  ventas_limpias.select("Fecha").distinct().show(5)
  ```
  ---
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img8.png)

#### Tarea 3.2.

- **Paso 1.** Calcula la columna `TotalNeto`:

  ```python
  %python
  from pyspark.sql.functions import col, round
  ventas_limpias = ventas_limpias.withColumn(
      "TotalNeto", round(col("Unidades") * col("PrecioUd") * (1 - col("Descuento") / 100), 2)
  )
  ventas_limpias.select("NumPed", "Unidades", "PrecioUd", "Descuento", "TotalNeto").show(10)
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img9.png)


- **Paso 2.** Extrae `Mes` y `Anio`:
    
  ```python
  %python
  from pyspark.sql.functions import month, year
  ventas_limpias = ventas_limpias.withColumn("Mes", month("Fecha")).withColumn("Anio", year("Fecha"))
  ventas_limpias.select("Fecha", "Mes", "Anio").show(10)
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img10.png)

> **TAREA FINALIZADA**

### Resultado esperado:
Contar치s con un DataFrame en PySpark con los datos limpios y enriquecidos, listo para el an치lisis.

---

### Tarea 4: Guardar versi칩n limpia con PySpark

Guardar치s los resultados procesados con PySpark en una nueva tabla Delta llamada `ventas_limpias`.

#### Tarea 4.1.

- **Paso 1.** Guarda la tabla:

  ```python
  %python
  ventas_limpias.write.format("delta").mode("overwrite").saveAsTable("ventas_limpias")
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img11.png)


- **Paso 2.** Valida desde SQL:

  ```sql
  SELECT COUNT(*), AVG(TotalNeto), MIN(Fecha), MAX(Fecha) FROM ventas_limpias;
  ```
  ---
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img12.png)

- **Paso 3.** Para validar desde la interfaz gr치fica, haz clic en la secci칩n **Catalog** del men칰 lateral izquierdo.

    ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img13.png)

- **Paso 4.** Si es necesario, conecta tu **`cluster al catalog`**, expande el metastore de Hive y ver치s la tabla que guardaste.

    ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img14.png)

- **Paso 5.** Regresa a tu **notebook** y crea una nueva celda debajo de la 칰ltima.

- **Paso 6.** Ejecuta el siguiente comando para escribir el contenido como un archivo CSV 칰nico en una carpeta con acceso p칰blico.

  ```python
  %python
  df = spark.table("ventas_limpias")
  # Ruta correcta para DBFS virtual (dbutils, navegaci칩n, descarga)
  df.coalesce(1).write.option("header", "true").mode("overwrite").csv("dbfs:/FileStore/ventas_limpias")
  ```
  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img21.png)

- **Paso 7.** Lista y extrae el nombre real del archivo:

  ```python
  %python
  files = dbutils.fs.ls("dbfs:/FileStore/ventas_limpias")

  for f in files:
      if f.name.endswith(".csv"):
          print(f.name)
  ```
  ---
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img22.png)

- **Paso 8.** Descarga el archivo f치cilmente generando un enlace directo desde el notebook.

  ```python
  %python
  workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
  filename = [f.name for f in dbutils.fs.ls("dbfs:/FileStore/ventas_limpias") if f.name.endswith(".csv")][0]
  displayHTML(f"<a href='https://{workspace_url}/files/ventas_limpias/{filename}' target='_blank'>游닌 Descargar CSV</a>")
  ```
  ---
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img23.png)

- **Paso 9.** Guarda el archivo con el nombre **`ventas_limpias`**.

> **TAREA FINALIZADA**

### Resultado esperado: 
Contar con una tabla optimizada y limpia, con transformaciones aplicadas usando PySpark, lista para an치lisis posteriores.

---

### Tarea 5: Transformaci칩n alternativa usando solo SQL  

Para analistas que prefieren trabajar exclusivamente con SQL, esta tarea ofrece un flujo equivalente a las tareas 2 y 3, usando 칰nicamente consultas SQL.

#### Tarea 5.1.

- **Paso 1.** Crea una vista filtrada que excluya los valores nulos y verifica el resultado.

  ```sql
  CREATE OR REPLACE TEMP VIEW ventas_filtradas AS
  SELECT * FROM ventas
  WHERE PrecioUd IS NOT NULL AND Unidades IS NOT NULL AND Fecha IS NOT NULL;
  ```

  ---

  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img15.png)

  ---
  
  ```sql
  SELECT * FROM ventas_filtradas LIMIT 10;
  ```

  ---
  
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img16.png)

  ---

- **Paso 2.** Crea una vista sin duplicados:  

  ```sql
  CREATE OR REPLACE TEMP VIEW ventas_deduplicadas AS
  SELECT DISTINCT * FROM ventas_filtradas;
  ```

  ---
  
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img17.png)

  ---
  
  ```sql
  SELECT * FROM ventas_deduplicadas LIMIT 10;
  ```

  ---
  
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img18.png)

#### Tarea 5.2.  

- **Paso 1.** Crea una tabla enriquecida con nuevas columnas: 

  ```sql
  CREATE OR REPLACE TABLE ventas_limpias_sql AS
  SELECT 
  *,
  ROUND(Unidades * PrecioUd * (1 - Descuento / 100), 2) AS TotalNeto,
  year(Fecha) AS Anio,
  month(Fecha) AS Mes,
  CASE WHEN Provincia = 'CDMX' THEN true ELSE false END AS EsCDMX
  FROM ventas_deduplicadas;
  SELECT * FROM ventas_limpias_sql LIMIT 10;
  SELECT COUNT(*) FROM ventas_limpias_sql;
  ```

  ---
  
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img19.png)

- **Paso 2.** Valida los registros:

  ```sql
  SELECT COUNT(*), AVG(TotalNeto), MIN(Fecha), MAX(Fecha) FROM ventas_limpias_sql;
  ```

  ---
  
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img20.png)

- **Paso 3.** Puedes repetir el **Paso 4** de la **Tarea 3** para visualizar la creaci칩n de la tabla desde la interfaz gr치fica.

- **Paso 4.** Ejecuta el siguiente comando que escribe el CSV como archivo 칰nico en una carpeta con acceso p칰blico.

  ```python
  %python
  df = spark.table("ventas_limpias_sql")
  # Ruta correcta para DBFS virtual (dbutils, navegaci칩n, descarga)
  df.coalesce(1).write.option("header", "true").mode("overwrite").csv("dbfs:/FileStore/ventas_limpias_sql")
  ```
  ---
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img24.png)

- **Paso 5.** Lista y extrae el nombre real del archivo:

  ```python
  %python
  files = dbutils.fs.ls("dbfs:/FileStore/ventas_limpias_sql")

  for f in files:
      if f.name.endswith(".csv"):
          print(f.name)
  ```
  ---
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img25.png)

- **Paso 6.** Descarga el archivo c칩modamente generando un enlace directo desde el notebook.

  ```python
  %python
  workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
  filename = [f.name for f in dbutils.fs.ls("dbfs:/FileStore/ventas_limpias_sql") if f.name.endswith(".csv")][0]
  displayHTML(f"<a href='https://{workspace_url}/files/ventas_limpias_sql/{filename}' target='_blank'>游닌 Descargar CSV</a>")
  ```
  ---
  ![dbricks2](/Custom_NETEC_DBRICKS-DA_INT-Priv/images/lab2/img26.png)

- **Paso 7.** Guarda el archivo con el nombre **`ventas_limpias_sql`**.

> **TAREA FINALIZADA**

### Resultado esperado:
Se ha generado una tabla llamada `ventas_limpias_sql` equivalente a la creada con PySpark, construida completamente con SQL y lista para su visualizaci칩n.

---

> **춰FELICIDADES! HAS COMPLETADO EL LABORATORIO 2.**

## Resultado final

Como analista, ahora cuentas con dos versiones limpias del dataset:

- `ventas_limpias`: transformada con PySpark. 
- `ventas_limpias_sql`: transformada con SQL.

Ambas est치n listas para visualizaci칩n, an치lisis estad칤stico o integraci칩n con dashboards. Con este laboratorio, reforzaste tu comprensi칩n de procesos ETL simples desde dos enfoques distintos, lo que te brinda mayor flexibilidad seg칰n tu experiencia t칠cnica.

---
**[拘勇 Atr치s](https://netec-mx.github.io/Custom_NETEC_DBRICKS-DA_INT-Priv/Cap칤tulo1/lab1.html)** | **[Lista General](https://netec-mx.github.io/Custom_NETEC_DBRICKS-DA_INT-Priv/)** | **[Siguiente 俱뫮잺](https://netec-mx.github.io/Custom_NETEC_DBRICKS-DA_INT-Priv/Cap칤tulo3/lab3.html)**
